{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('datas_positive.json','r',encoding='utf-8') as f:\n",
    "  datas_positive = json.load(f)\n",
    "with open ('datas_negative.json','r',encoding='utf-8') as f:\n",
    "  datas_negative = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "'basic' :\"\"\"请仔细阅读以下多条文本,然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "\"\"\",\n",
    "'OR': \"\"\"请仔细阅读以下多条文本，关注用户的情绪表达、对未来的态度、社交参与度、兴趣水平以及任何关于身体症状或睡眠/食欲变化、自我价值、个人感受的表述。回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，解释你为什么给出这个答案。\"\"\",\n",
    "'OR2': \"\"\"请仔细阅读以下多条文本然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，解释你为什么给出这个答案。\"\"\",\n",
    "'OR3': \"\"\"请仔细阅读以下多条文本然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "\"\"\",\n",
    "\n",
    "'RET':\"\"\"请逐条阅读以下用户的文本，关注用户的情绪表达、对未来的态度、社交参与度、兴趣水平以及任何关于身体症状或睡眠/食欲变化、自我价值、个人感受的表述。回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程，指出哪条或哪几条文本让你得出这个回答。\n",
    "***重要***\n",
    "如果用户在文本中明确提到自己得到了抑郁的诊断，请直接判断为“是”，并在分析中指出相关文本。\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，指出哪条或哪几条文本让你得出这个回答。\"\"\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_tweet (tweet,mode = 1):\n",
    "    input_text_list = tweet.split('\\n')\n",
    "    if mode == 0 :\n",
    "        tweet = tweet\n",
    "    elif mode == 1:\n",
    "        input_text_list_mod= [f'文本{i+1}:{text}' for i,text in enumerate(input_text_list)]\n",
    "        tweet = '\\n'.join(input_text_list_mod)\n",
    "    elif mode == 2:\n",
    "        input_text_list_mod = [f'<文本{i+1}>{text}</文本{i+1}> ' for i,text in enumerate(input_text_list)]\n",
    "        tweet = '\\n'.join(input_text_list_mod)\n",
    "    return tweet\n",
    "def rmsnorm(x, eps=1e-6):\n",
    "    \"\"\"\n",
    "    手动实现RMSNorm归一化操作\n",
    "    :param x: 输入张量\n",
    "    :param eps: 防止除零的小常数\n",
    "    :return: 归一化后的张量\n",
    "    \"\"\"\n",
    "    rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)\n",
    "    return x / rms\n",
    "\n",
    "def find_start_end_index(start_token = '<|im_start|>',end_token ='<|im_end|>',drift = 3):\n",
    "    input_start = 0\n",
    "    input_end = 0\n",
    "    output_start = 0\n",
    "    output_end = 0\n",
    "    input_count = 0\n",
    "    output_count = 0\n",
    "    i = 0\n",
    "    for token in generated_ids[0][0]:\n",
    "        if tokenizer.decode(token) == start_token:\n",
    "            if input_count == 0:\n",
    "                input_start = i+drift\n",
    "                input_count += 1\n",
    "            else:\n",
    "                output_start = i+drift\n",
    "        elif tokenizer.decode(token) == end_token:\n",
    "            if output_count == 0:\n",
    "                input_end = i\n",
    "                output_count += 1\n",
    "            else:\n",
    "                output_end = i\n",
    "        i += 1\n",
    "    print('input_start:',input_start,'input_end:',input_end)\n",
    "    print('output_start:',output_start,'output_end:',output_end)\n",
    "    assert input_start < input_end\n",
    "    #assert output_start < output_end\n",
    "    return input_start,input_end,output_start,output_end\n",
    "\n",
    "def compute_attention_rollout(converted_attentions,response,norm_type = 'RMS',add_residual=True, eps=1e-6):\n",
    "    all_attentions =  [(t,a) for t,a in zip(response.tolist(), converted_attentions)]\n",
    "    joint_attentions =[]\n",
    "    for pair in all_attentions:\n",
    "        token = pair[0]\n",
    "        res_att_mat = pair[1].mean(axis=1) #平均注意力头的权重，得到一个 [layer=32, seq_len=66或者1, seq_len=66或者66+生成步数] 的张量\n",
    "        if add_residual:\n",
    "            res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...] #残差连接\n",
    "        if norm_type == 'RMS':\n",
    "            res_att_mat = rmsnorm(torch.tensor(res_att_mat), eps=eps).numpy() #RMSNorm归一化\n",
    "        elif norm_type == 'layernorm':\n",
    "            res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None] #LayerNorm归一化\n",
    "        res_att_mat = rmsnorm(torch.tensor(res_att_mat), eps=eps).numpy()\n",
    "        joint_att = np.zeros(res_att_mat.shape)\n",
    "        layers = joint_att.shape[0]\n",
    "        joint_att[0] = res_att_mat[0]\n",
    "        for i in np.arange(1, layers):\n",
    "            joint_att[i] = res_att_mat[i].dot(joint_att[i-1].T)\n",
    "        joint_attentions.append((token,joint_att))\n",
    "    return joint_attentions\n",
    "\n",
    "def generate_text_with_scores_html(tensor, text, output_path, normalize=True, method='min-max', window_size=10):\n",
    "    \"\"\"\n",
    "    根据输入的向量和文本生成一个HTML文件，其中每个字符上方显示对应的分数，\n",
    "    并根据分数调整颜色。最终的HTML文件保存在指定路径中。\n",
    "\n",
    "    参数:\n",
    "    tensor (numpy.ndarray): 形状为 (1, N) 的向量，其中 N 是文本的长度。\n",
    "    text (str): 文本字符串，与向量长度匹配。\n",
    "    output_path (str): 输出HTML文件的路径。\n",
    "    normalize (bool): 是否归一化分数，默认为True。\n",
    "    method (str): 归一化方法，支持 'min-max'（默认）、'mean' 和 'moving-average'。\n",
    "    window_size (int): 滑动平均窗口大小，默认为10，仅在选择滑动平均归一化时使用。\n",
    "\n",
    "    返回:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # 检查张量和文本长度是否匹配\n",
    "    assert tensor.shape[1] == len(text), \"张量和文本的长度不匹配！\"\n",
    "\n",
    "    scores = tensor[0]\n",
    "\n",
    "    # 归一化分数\n",
    "    if normalize:\n",
    "        if method == 'min-max':\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        elif method == 'mean':\n",
    "            mean_value = scores.mean()\n",
    "            scores = scores - mean_value\n",
    "        elif method == 'moving-average':\n",
    "            mean_value = np.convolve(scores, np.ones(window_size) / window_size, mode='same')\n",
    "            scores = scores - mean_value\n",
    "        else:\n",
    "            raise ValueError(\"未知的归一化方法。请使用 'min-max', 'mean', 或 'moving-average'。\")\n",
    "\n",
    "    # 创建颜色映射函数\n",
    "    def score_to_color(score):\n",
    "        r = int(255 * score)\n",
    "        b = 255 - r\n",
    "        return f'rgb({r}, 0, {b})'\n",
    "\n",
    "    # 定义每行显示的字符数\n",
    "    chars_per_line = 50\n",
    "    num_lines = len(text) // chars_per_line + (1 if len(text) % chars_per_line else 0)\n",
    "\n",
    "    # 生成HTML内容\n",
    "    html_content = \"<html><body style='font-family:monospace;'>\\n\"\n",
    "\n",
    "    # 设置间隔\n",
    "    spacing = \"20px\"  # 可以根据需要调整间隔\n",
    "\n",
    "    for line in range(num_lines):\n",
    "        start_idx = line * chars_per_line\n",
    "        end_idx = start_idx + chars_per_line\n",
    "        line_text = text[start_idx:end_idx]\n",
    "        line_scores = scores[start_idx:end_idx]\n",
    "\n",
    "        for i, char in enumerate(line_text):\n",
    "            color = score_to_color(line_scores[i])\n",
    "            score_text = f\"{line_scores[i]:.2f}\"\n",
    "            border_style = \"border: 1px solid black;\" if line_scores[i] > 0 else \"\"\n",
    "            html_content += f\"<div style='display:inline-block; text-align:center; color:{color}; margin-right:{spacing}; {border_style}'>\" \\\n",
    "                            f\"<div style='font-size:0.5em;'>{score_text}</div>\" \\\n",
    "                            f\"<div>{char}</div>\" \\\n",
    "                            f\"</div>\"\n",
    "\n",
    "        html_content += \"<br>\\n\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # 保存为HTML文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML文件已生成并保存在 {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama3-8B-Chinese-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype= torch.bfloat16, device_map=\"auto\",attn_implementation=\"eager\", \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"判断以下文本是否表达了抑郁情绪：'我觉得生活毫无意义，每一天都很痛苦。'\"\n",
    "#prompt = system_prompt['RET']+'\\n'+modify_tweet(tweet_tp,mode=1)\n",
    "prompt = system_prompt['OR2'] + '\\n' + tweet_x\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"system\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"system\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_params = {'max_new_tokens': 100,\n",
    "                     'pad_token_id': tokenizer.pad_token_id,\n",
    "                     'terminator_ids': terminators,\n",
    "                     'do_sample': True,\n",
    "                     'output_attentions': True,\n",
    "                     'return_dict_in_generate': True,\n",
    "                     #'temperature': 0.5,\n",
    "                    #'num_return_sequences': 1,\n",
    "                     #'top_k': 50,\n",
    "                     #'top_p': 0.95,\n",
    "                    # 'repetition_penalty': 1.2,\n",
    "                     #'do_sample': True,\n",
    "                     #'no_repeat_ngram_size': 3,\n",
    "                     #'max_length': 100,\n",
    "                     #'use_cache': True\n",
    "        }\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    **generation_params\n",
    "    )\n",
    "response_ids = generated_ids[0][0][input_ids.shape[-1]:]\n",
    "response = tokenizer.decode(response_ids, skip_special_tokens=False)\n",
    "print(response)\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算输入序列每个token总的输出输入注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index,end_index,_,_ = find_start_end_index(start_token='<|end_header_id|>',end_token='<|eot_id|>',drift=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = start_index\n",
    "t = end_index\n",
    "attention_input = [\n",
    "    torch.max(hd[:, :, s:t, s:t].cpu().squeeze(0).to(torch.float32), dim=0)[0]\n",
    "    for hd in generated_ids['attentions'][0]\n",
    "]\n",
    "print(len(attention_input))\n",
    "print(attention_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviced_attention = []\n",
    "attender_attention = []\n",
    "recived_pairs = []\n",
    "index_tokens = [ tokenizer.decode(t,skip_special_tokens=False) for t in generated_ids[0][0][s:t]]\n",
    "seq_len = len(index_tokens)\n",
    "for layer in attention_input:\n",
    "    total_attention_as_object = torch.sum(layer, axis=0).tolist()\n",
    "    total_attention_as_object = [att/seq_len for att in total_attention_as_object]\n",
    "    total_attention_as_attender = torch.sum(layer, axis=1).tolist()\n",
    "    total_attention_as_attender = [att/seq_len for att in total_attention_as_attender]\n",
    "    reviced_attention.append(total_attention_as_object)\n",
    "    attender_attention.append(total_attention_as_attender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for layer in reviced_attention:\n",
    "    recived_pairs.append([(token,att) for token,att in zip(index_tokens,layer)])\n",
    "    print(f'layer{i}:{sorted(recived_pairs[-1],key=lambda x:x[1],reverse=True)[:10]}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviced_attention_tensor = [np.array(layer) for layer in reviced_attention]\n",
    "bias_vector = np.ones(reviced_attention_tensor[0].shape[0])\n",
    "bias_vector = bias_vector[None, :]\n",
    "reviced_attention_tensor = np.array(reviced_attention_tensor) + bias_vector\n",
    "reviced_attention_tensor = rmsnorm(torch.tensor(reviced_attention_tensor)).numpy()   \n",
    "layers = reviced_attention_tensor.shape[0]\n",
    "joint_attention = np.zeros(reviced_attention_tensor.shape)\n",
    "joint_attention[0] = reviced_attention_tensor[0]\n",
    "for i in np.arange(1, layers):\n",
    "    joint_attention[i] = reviced_attention_tensor[i] * joint_attention[i-1]  #逐元素相乘,因为原文中的公式是用于计算注意力方阵的[seq_len,seq_len]，但是这里用于计算注意力向量[1,seq_len]，所以需要逐元素相乘\n",
    "last_layer_self_rollout = joint_attention[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('生成步骤:',type(generated_ids['attentions']),len(generated_ids['attentions']))\n",
    "print('每一步包含的层数：',type(generated_ids['attentions'][0]),len(generated_ids['attentions'][0]))\n",
    "print('每一层中的注意力矩阵:',type(generated_ids['attentions'][0][0]),generated_ids['attentions'][0][0].shape)\n",
    "#适用于未经过形态转换的注意力权重,其格式如以上所示\n",
    "all_attentions =  [(t,a) for t,a in zip(response_ids.tolist(), generated_ids['attentions'])] #attention dict for all tokens\n",
    "all_attentions_avg = [(t,sum(a)/len(a)) for t,a in zip(response_ids.tolist(), generated_ids['attentions'])] #average attention dict for all tokens\n",
    "print(all_attentions_avg[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(generated_ids['attentions'])):\n",
    "    for j in range(len(generated_ids['attentions'][i])):\n",
    "        print(f'生成步数{i+1},层数{j+1}',generated_ids['attentions'][i][j].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 attentions_step1 是第 1 步的注意力张量，长度为 28 的 tuple\n",
    "attentions_step1 = generated_ids['attentions'][0]\n",
    "\n",
    "# 创建一个列表用于存储每一层的最后一个 token 的注意力\n",
    "last_token_attention_list = []\n",
    "assert generated_ids[0][0][end_index:end_index+1] == tokenizer.eos_token_id\n",
    "# 遍历 tuple 中的每一层注意力矩阵\n",
    "for layer_attention in attentions_step1:\n",
    "    # 选择输出最后一个 token （ <|im_end|>）的注意力，并添加到列表中\n",
    "    last_token_attention = layer_attention[:, :, end_index:end_index+1:, :]\n",
    "    last_token_attention_list.append(last_token_attention)\n",
    "\n",
    "# 你可以将这个列表转为 tensor 或者继续保留为列表\n",
    "# last_token_attention_tensor = torch.stack(last_token_attention_list)\n",
    "\n",
    "# 查看第一个层的最后一个 token 注意力的形状\n",
    "print(last_token_attention_list[0].shape)\n",
    "print(len(last_token_attention_list))\n",
    "attentions_list = list(generated_ids['attentions'])\n",
    "\n",
    "# 将生成步骤 0 对应的 tuple 替换为新的 tuple\n",
    "attentions_list[0] = tuple(last_token_attention_list)\n",
    "\n",
    "# 将列表转换回 tuple 并赋值回 generated_ids['attentions']\n",
    "generated_ids['attentions'] = tuple(attentions_list)\n",
    "\n",
    "# 验证替换后的内容\n",
    "print(type(generated_ids['attentions']))  # 应该是 tuple\n",
    "print(generated_ids['attentions'][0][0].shape)  # 应该是 [1, 28, 1, 1776]\n",
    "\n",
    "\n",
    "# 创建一个新的列表来存储转换后的张量\n",
    "# 假如输入长度为66，这个数字包含了特殊符号和系统信息，prompt从第54个位置开始\n",
    "converted_attentions = []\n",
    "'''\n",
    "目前find_start_end_nb函数还有问题，所以这里手动指定start_index和end_index\n",
    "start_index, end_index = find_start_end_nb(text, trigger_word='user')\n",
    "end_index = end_index + 1\n",
    "'''\n",
    "#这里的start_index和end_index代表的是输入序列的起始和结束位置\n",
    "\n",
    "for i in range(len(generated_ids['attentions'])):\n",
    "    # 将每个生成步骤中的 28 层中 [batch_size = 1, attention_head=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数] 张量拼接成一个 [layer=28, attention_head=28, seq_len=输入长度或者1, seq_len=(输入长度或者输入长度+生成步数] 的张量\n",
    "    step_attentions = torch.cat([layer for layer in generated_ids['attentions'][i]], dim=0)\n",
    "    \n",
    "    # 保留从start_index到end_index之间的内容\n",
    "    if step_attentions.shape[-1] > start_index:\n",
    "        step_attentions = step_attentions[:, :, :, start_index:end_index]\n",
    "    \n",
    "    converted_attentions.append(step_attentions.detach().clone().cpu().to(torch.float32).numpy())\n",
    "\n",
    "# 打印转换后的张量形状\n",
    "for i, tensor in enumerate(converted_attentions):\n",
    "    print(f\"Shape of tensor at step {i+1}: {tensor.shape}\")  # 应该输出 [layer=28, attention_head=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数]，对于最后一个seq_len，只保留从start_index到end_index之间的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算RAW ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attentions =  [(t,a) for t,a in zip(response_ids[0].tolist(), converted_attentions)]\n",
    "all_attentions_avg =[]\n",
    "for pair in all_attentions:\n",
    "    token = pair[0]\n",
    "    #res_att_mat = pair[1].mean(axis=1) #平均注意力头的权重，得到一个 [layer=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数] \n",
    "    res_att_mat = pair[1].max(axis=1) #取最大注意力头的权重，得到一个 [layer=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数]\n",
    "    bias_vector = np.ones(pair[1].shape[-1])\n",
    "    bias_vector = bias_vector[None, None, :]\n",
    "    res_att_mat = res_att_mat + bias_vector #对[layers,1,seq_len]做残差连接\n",
    "    #res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None] #归一化\n",
    "    res_att_mat = rmsnorm(torch.tensor(res_att_mat)).numpy() #RMSNorm归一化\n",
    "    all_attentions_avg.append((token, res_att_mat)) #token是当前生成的token，res_att_mat是当前生成的token对输入token的注意力权重\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算Attention Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "all_attentions =  [(t,a) for t,a in zip(response_ids.tolist(), converted_attentions)]\n",
    "joint_attentions =[]\n",
    "for pair in all_attentions:\n",
    "    token = pair[0]\n",
    "    #res_att_mat = pair[1].mean(axis=1) #平均注意力头的权重，得到一个 [layer=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数] 的张量\n",
    "    res_att_mat = pair[1].max(axis=1) #取最大注意力头的权重，得到一个 [layer=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数]\n",
    "    #res_att_mat = pair[1].min(axis=1) #取最小注意力头的权重，得到一个 [layer=28, seq_len=输入长度或者1, seq_len=输入长度或者输入长度+生成步数]\n",
    "    #res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...] #对[layers,seq_len,seq_len]做残差连接\n",
    "    #res_att_mat = np.eye(res_att_mat.shape[2])[None, None, ...]\n",
    "    bias_vector = np.ones(pair[1].shape[-1])\n",
    "    bias_vector = bias_vector[None, None, :]\n",
    "    res_att_mat = res_att_mat + bias_vector #对[layers,1,seq_len]做残差连接\n",
    "    res_att_mat = rmsnorm(torch.tensor(res_att_mat), eps=eps).numpy() #RMSNorm归一化\n",
    "    #res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None] #归一化\n",
    "    joint_att = np.zeros(res_att_mat.shape)\n",
    "    layers = joint_att.shape[0]\n",
    "    joint_att[0] = res_att_mat[0]\n",
    "    for i in np.arange(1, layers):\n",
    "        joint_att[i] = res_att_mat[i] * joint_att[i-1]  #逐元素相乘,因为原文中的公式是用于计算注意力方阵的[seq_len,seq_len]，但是这里用于计算注意力向量[1,seq_len]，所以需要逐元素相乘\n",
    "    joint_attentions.append((token,joint_att))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算所有层的注意力最大值/平均值/最后一层，并且输出HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最后层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = []\n",
    "seq_ids = (model_inputs[0].tolist())[start_index:end_index]\n",
    "seq_tokens = [tokenizer.decode([token]) for token in seq_ids] #输入序列的token\n",
    "all_generated_joint_attentions_avg = np.array([att[1] for att in joint_attentions]).mean(axis=0) #计算所有生成步数的平均注意力rollout\n",
    "last_layer = all_generated_joint_attentions_avg[-1] #取最后一层的注意力\n",
    "last_layer = last_layer+last_layer_self_rollout.reshape(1,-1) \n",
    "print(last_layer.mean())\n",
    "print(len(seq_tokens))\n",
    "generate_text_with_scores_html(last_layer,seq_tokens,normalize= True,method='mean',output_path=f'llama3_Roll_maxhead_lastlayer_self_att_correct__TEST1.html')##### 最后层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最大层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大值\n",
    "seq_ids = []\n",
    "seq_ids = (model_inputs.input_ids[0].tolist())[start_index:end_index]\n",
    "seq_tokens = [tokenizer.decode([token]) for token in seq_ids] #输入序列的token\n",
    "all_generated_joint_attentions_avg = np.array([att[1] for att in joint_attentions]).mean(axis=0) #计算所有生成步数的平均注意力rollout\n",
    "all_layer_max = all_generated_joint_attentions_avg.max(axis=0) #取所有层的最大值\n",
    "print(all_layer_max.mean())\n",
    "generate_text_with_scores_html(all_layer_max,seq_tokens,normalize= True,method='mean',output_path=f'Roll_layermax_tp_correct_{prefix}2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 平均层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = []\n",
    "seq_ids = (model_inputs.input_ids[0].tolist())[start_index:end_index]\n",
    "seq_tokens = [tokenizer.decode([token]) for token in seq_ids] #输入序列的token\n",
    "all_generated_joint_attentions_avg = np.array([att[1] for att in joint_attentions]).mean(axis=0) #计算所有生成步数的平均注意力rollout\n",
    "all_layer_mean = all_generated_joint_attentions_avg.mean(axis=0) #取所有层的平均值\n",
    "print(all_layer_mean.mean())\n",
    "generate_text_with_scores_html(all_layer_mean,seq_tokens,normalize= True,method='mean',output_path=f'Roll_layermean_tp_correct_{prefix}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 累加注意力动画 (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_avg_attentions = []\n",
    "response_per_token = [tokenizer.decode([token]) for token in response_ids[0]]\n",
    "# 逐步累加生成步骤的平均注意力\n",
    "for step in range(len(joint_attentions)):\n",
    "    # 取前 step+1 个生成步的注意力\n",
    "    avg_attention = np.array([att[1] for att in joint_attentions[:step+1]]).mean(axis=0)[-1]\n",
    "    # 将当前步骤的累积平均注意力加入列表\n",
    "    cumulative_avg_attentions.append(avg_attention)\n",
    "last_layer_self_rollout = last_layer_self_rollout.reshape(1,-1)\n",
    "last_layer_self_rollout = (last_layer_self_rollout - last_layer_self_rollout.mean(axis=0))/last_layer_self_rollout.std(axis=0)  \n",
    "cumulative_avg_attentions = [((att - att.mean())/att.std()).tolist()[0] for att in cumulative_avg_attentions]\n",
    "cumulative_avg_attentions = [att + last_layer_self_rollout for att in cumulative_avg_attentions]\n",
    "\n",
    "with open('attention_data.js', 'w') as f:\n",
    "    f.write(f\"const seq_tokens = {json.dumps(seq_tokens)};\\n\")\n",
    "    f.write(f\"const cumulative_avg_attentions = {json.dumps(cumulative_avg_attentions)};\\n\")\n",
    "    f.write(f\"const response_per_token = {json.dumps(response_per_token)};\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
